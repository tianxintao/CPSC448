{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cliff Walking#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I mainly used the code accompanying the RL book, and I added double Q learning to see if there is any difference. Also, I tried different step size for different methods** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cliff_walking import CliffWalking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliffWalking = CliffWalking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 11] [3, 0]\n"
     ]
    }
   ],
   "source": [
    "print(cliffWalking.GOAL,cliffWalking.START)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarsa Optimal Policy:\n",
      "['R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'D']\n",
      "['U', 'L', 'U', 'U', 'L', 'U', 'U', 'U', 'L', 'U', 'R', 'D']\n",
      "['U', 'U', 'U', 'U', 'U', 'U', 'R', 'U', 'U', 'U', 'U', 'D']\n",
      "['U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'G']\n"
     ]
    }
   ],
   "source": [
    "cliffWalking.TestSarsa()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Which means the optimal route starts from the buttom left -> top left -> top right -> buttom right*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Sarsa Optimal Policy:\n",
      "['R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'D']\n",
      "['U', 'U', 'U', 'U', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'D']\n",
      "['U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'R', 'D']\n",
      "['U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'G']\n"
     ]
    }
   ],
   "source": [
    "cliffWalking.TestExpectedSarsa()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The optimal route is the same as the one got from Sarsa*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Learning Optimal Policy:\n",
      "['D', 'R', 'R', 'R', 'R', 'R', 'R', 'D', 'R', 'D', 'D', 'D']\n",
      "['D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D']\n",
      "['R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'D']\n",
      "['U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'G']\n"
     ]
    }
   ],
   "source": [
    "cliffWalking.TestQLearning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The optimal route from Q learning: go up once, then go right until the end, then go down*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Double-Q-Learning Optimal Policy:\n",
      "['D', 'D', 'D', 'D', 'R', 'R', 'R', 'D', 'D', 'R', 'R', 'D']\n",
      "['R', 'D', 'R', 'D', 'D', 'D', 'D', 'D', 'D', 'R', 'D', 'D']\n",
      "['R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'D']\n",
      "['U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'G']\n"
     ]
    }
   ],
   "source": [
    "cliffWalking.TestDoubleQLearning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The route from double Q-learning is the same as Q learning. This makes sense because double learning is used to solve maximization bias. Based on what I understand, maximization bias is not a problem in deterministic environment with a variance of 0* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observation during coding:\n",
    "1. The computation of each episode of Sarsa is quick. However, the action-value function of Sarsa is way harder to converge compared with expected Sarsa. This makes sense because expected Sarsa uses the expected action-value function of next state rather than just one action-value pair chosen by the policy\n",
    "2. On the RL book, there is a statement in Chapter 6.6 \"In addition, Expected Sarsa shows a significant improvement over Sarsa over a wide range of values for the step-size parameter\". This is true, as I noticed that Sarsa is very picky with the step size and discount factor. When I set discount factor to be 1, it seems to never converge.\n",
    "3. Double Q-learning converges way slower than regular Q learning as it is trying to make two action-value function converges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try changing the penalty of droping the cliff to -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliffWalking = CliffWalking(penalty=-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarsa Optimal Policy:\n",
      "['R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'D']\n",
      "['U', 'U', 'U', 'L', 'U', 'U', 'U', 'U', 'L', 'U', 'R', 'D']\n",
      "['U', 'U', 'R', 'R', 'U', 'U', 'U', 'U', 'U', 'U', 'R', 'D']\n",
      "['U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'G']\n"
     ]
    }
   ],
   "source": [
    "cliffWalking.TestSarsa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Sarsa Optimal Policy:\n",
      "['R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'D']\n",
      "['R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'D']\n",
      "['U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'R', 'R', 'R', 'D']\n",
      "['U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'G']\n"
     ]
    }
   ],
   "source": [
    "cliffWalking.TestExpectedSarsa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Learning Optimal Policy:\n",
      "['R', 'R', 'D', 'R', 'R', 'R', 'D', 'R', 'R', 'D', 'D', 'D']\n",
      "['D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D']\n",
      "['R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'D']\n",
      "['U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'G']\n"
     ]
    }
   ],
   "source": [
    "cliffWalking.TestQLearning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Double-Q-Learning Optimal Policy:\n",
      "['R', 'R', 'R', 'D', 'D', 'D', 'R', 'R', 'R', 'D', 'D', 'D']\n",
      "['R', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D']\n",
      "['R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'D']\n",
      "['U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'G']\n"
     ]
    }
   ],
   "source": [
    "cliffWalking.TestDoubleQLearning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Among the 4 methods, only the expected Sarsa changes the route. Maybe we can discuss the cause of it on Thursday?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
